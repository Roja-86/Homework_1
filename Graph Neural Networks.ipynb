{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries and dataset path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Roja\\AppData\\Local\\Temp\\ipykernel_13528\\564548303.py:12: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n",
      "  set_matplotlib_formats('svg', 'pdf') # For export\n",
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "## Standard libraries\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "## Imports for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg', 'pdf') # For export\n",
    "from matplotlib.colors import to_rgb\n",
    "import matplotlib\n",
    "matplotlib.rcParams['lines.linewidth'] = 2.0\n",
    "import seaborn as sns\n",
    "sns.reset_orig()\n",
    "sns.set()\n",
    "\n",
    "## Progress bar\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "## PyTorch (Main library for building and training neural networks)\n",
    "import torch  # Core PyTorch library for tensors and operations on them.\n",
    "import torch.nn as nn  # nn contains classes for creating and defining neural network layers.\n",
    "import torch.nn.functional as F  # F contains functions that perform operations such as activations, convolutions, etc., without needing to define layers explicitly.\n",
    "import torch.utils.data as data  # Utilities for loading and managing data (like DataLoader).\n",
    "import torch.optim as optim  # Contains optimization algorithms such as SGD, Adam for training models.\n",
    "\n",
    "## Torchvision (Used for loading common datasets and performing image transformations)\n",
    "import torchvision  # Provides utilities for computer vision tasks such as datasets, models, and transformations.\n",
    "from torchvision.datasets import CIFAR10  # CIFAR10 is a commonly used dataset containing small 32x32 color images across 10 classes.\n",
    "from torchvision import transforms  # Used to apply transformations (e.g., resize, normalize) to the input data.\n",
    "\n",
    "## PyTorch Lightning (A framework that simplifies training, testing, and validation of models)\n",
    "try:\n",
    "    import pytorch_lightning as pl  # PyTorch Lightning (pl) is an abstraction layer that organizes PyTorch code, making it easier to manage experiments and models.\n",
    "except ModuleNotFoundError:  # If PyTorch Lightning is not installed (e.g., in Google Colab environments), the script installs it.\n",
    "    !pip install --quiet pytorch-lightning>=1.4  # Installing PyTorch Lightning (version >= 1.4) if it's missing.\n",
    "    import pytorch_lightning as pl  # After installing, it imports PyTorch Lightning.\n",
    "\n",
    "# Callbacks from PyTorch Lightning\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint  \n",
    "# LearningRateMonitor tracks the learning rate during training.\n",
    "# ModelCheckpoint saves the best model during training, based on some criteria (e.g., lowest validation loss).\n",
    "\n",
    "# Path to the folder where the datasets are/should be downloaded (e.g., CIFAR10)\n",
    "DATASET_PATH = \"D:/USC_Course/CSCE 790 Section 007 Neural Networks and Their Applications/cifar-10-python/cifar-10-batches-py\"\n",
    "# This specifies the local path where the CIFAR-10 dataset will be downloaded or loaded from.\n",
    "\n",
    "# Path to the folder where the pretrained models are saved\n",
    "CHECKPOINT_PATH = \"D:/USC_Course/CSCE 790 Section 007 Neural Networks and Their Applications/CHECKPOINT_PATH\"\n",
    "# This path specifies where the trained or pre-trained models will be saved or loaded from.\n",
    "\n",
    "# Setting the seed for reproducibility\n",
    "pl.seed_everything(42)  # This sets the random seed to ensure results are reproducible. The number 42 is arbitrary, but commonly used in random seeds.\n",
    "\n",
    "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.deterministic = True  # Ensures that the results on GPU are deterministic by disabling non-deterministic algorithms.\n",
    "torch.backends.cudnn.benchmark = False  # Disabling benchmarking makes training slower but ensures deterministic behavior.\n",
    "\n",
    "# Check if GPU is available, otherwise use CPU\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "# \"cuda:0\" refers to the first GPU, if available. If no GPU is found, it will use the CPU for computations.\n",
    "\n",
    "print(device)  # Print the device being used (either \"cuda:0\" for GPU or \"cpu\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-trained models which is downloadable from the mentioned URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from urllib.error import HTTPError\n",
    "# Github URL where saved models are stored for this tutorial\n",
    "base_url = \"https://raw.githubusercontent.com/phlippe/saved_models/main/tutorial7/\"\n",
    "# Files to download\n",
    "CHECKPOINT_PATH = \"D:/USC_Course/CSCE 790 Section 007 Neural Networks and Their Applications/CHECKPOINT_PATH/\"\n",
    "pretrained_files = [\"NodeLevelMLP.ckpt\", \"NodeLevelGNN.ckpt\", \"GraphLevelGraphConv.ckpt\"]\n",
    "\n",
    "# Create checkpoint path if it doesn't exist yet\n",
    "os.makedirs(CHECKPOINT_PATH, exist_ok=True)\n",
    "\n",
    "# For each file, check whether it already exists. If not, try downloading it.\n",
    "for file_name in pretrained_files:\n",
    "    file_path = os.path.join(CHECKPOINT_PATH, file_name)\n",
    "    if \"/\" in file_name:\n",
    "        os.makedirs(file_path.rsplit(\"/\",1)[0], exist_ok=True)\n",
    "    if not os.path.isfile(file_path):\n",
    "        file_url = base_url + file_name\n",
    "        print(f\"Downloading {file_url}...\")\n",
    "        try:\n",
    "            urllib.request.urlretrieve(file_url, file_path)\n",
    "        except HTTPError as e:\n",
    "            print(\"Something went wrong. Please try to download the file from the GDrive folder, or contact the author with the full output including the following error:\\n\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of the GCNLayer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, c_in, c_out):\n",
    "        super().__init__()\n",
    "        # Linear transformation to map input features to output features\n",
    "        self.projection = nn.Linear(c_in, c_out)\n",
    "\n",
    "    def forward(self, node_feats, adj_matrix):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            node_feats - Tensor with node features of shape [batch_size, num_nodes, c_in]\n",
    "            adj_matrix - Batch of adjacency matrices of the graph. If there is an edge from i to j, adj_matrix[b,i,j]=1 else 0.\n",
    "                         Supports directed edges by non-symmetric matrices. Assumes to already have added the identity connections.\n",
    "                         Shape: [batch_size, num_nodes, num_nodes]\n",
    "        \"\"\"\n",
    "        # Calculate number of neighbors by summing adjacency matrix along the last dimension\n",
    "        num_neighbours = adj_matrix.sum(dim=-1, keepdims=True)\n",
    "        \n",
    "        # Apply linear transformation to node features\n",
    "        node_feats = self.projection(node_feats)\n",
    "        \n",
    "        # Perform graph convolution (message passing) by matrix multiplication between adj_matrix and node features\n",
    "        node_feats = torch.bmm(adj_matrix, node_feats)\n",
    "        \n",
    "        # Normalize the updated node features by the number of neighbors (degree normalization)\n",
    "        node_feats = node_feats / num_neighbours\n",
    "        \n",
    "        # Return the updated node features\n",
    "        return node_feats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing Node Features and Adjacency Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node features:\n",
      " tensor([[[0., 1.],\n",
      "         [2., 3.],\n",
      "         [4., 5.],\n",
      "         [6., 7.]]])\n",
      "\n",
      "Adjacency matrix:\n",
      " tensor([[[1., 1., 0., 0.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [0., 1., 1., 1.],\n",
      "         [0., 1., 1., 1.]]])\n"
     ]
    }
   ],
   "source": [
    "# Create a tensor of node features with values ranging from 0 to 7 and cast to float32 type\n",
    "node_feats = torch.arange(8, dtype=torch.float32).view(1, 4, 2)\n",
    "# The tensor is reshaped to [1, 4, 2] where:\n",
    "# 1 is the batch size,\n",
    "# 4 is the number of nodes,\n",
    "# 2 is the number of features per node.\n",
    "\n",
    "# Define an adjacency matrix representing the graph structure\n",
    "adj_matrix = torch.Tensor([[[1, 1, 0, 0],  # Node 0 is connected to node 1\n",
    "                            [1, 1, 1, 1],  # Node 1 is connected to all nodes\n",
    "                            [0, 1, 1, 1],  # Node 2 is connected to node 1, 2, and 3\n",
    "                            [0, 1, 1, 1]]]) # Node 3 is connected to node 1, 2, and 3\n",
    "# Shape of adj_matrix: [1, 4, 4], indicating 1 batch, 4 nodes, and connections between nodes\n",
    "\n",
    "# Print node features tensor to verify the initialized values\n",
    "print(\"Node features:\\n\", node_feats)\n",
    "\n",
    "# Print adjacency matrix to verify the defined graph structure\n",
    "print(\"\\nAdjacency matrix:\\n\", adj_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying a GCN Layer with Identity Weight Matrix to Simplify Message Passing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjacency matrix tensor([[[1., 1., 0., 0.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [0., 1., 1., 1.],\n",
      "         [0., 1., 1., 1.]]])\n",
      "Input features tensor([[[0., 1.],\n",
      "         [2., 3.],\n",
      "         [4., 5.],\n",
      "         [6., 7.]]])\n",
      "Output features tensor([[[1., 2.],\n",
      "         [3., 4.],\n",
      "         [4., 5.],\n",
      "         [4., 5.]]])\n"
     ]
    }
   ],
   "source": [
    "# Initialize a GCN layer with 2 input features (c_in) and 2 output features (c_out)\n",
    "layer = GCNLayer(c_in=2, c_out=2)\n",
    "\n",
    "# Set the linear transformation weight matrix to an identity matrix (no transformation on features)\n",
    "layer.projection.weight.data = torch.Tensor([[1., 0.], [0., 1.]])\n",
    "# This ensures that the input features remain unchanged during the linear transformation.\n",
    "\n",
    "# Set the bias term of the linear transformation to zero\n",
    "layer.projection.bias.data = torch.Tensor([0., 0.])\n",
    "# No bias will be added to the input features, further simplifying the transformation.\n",
    "\n",
    "# Perform a forward pass through the GCN layer without computing gradients (for efficiency)\n",
    "with torch.no_grad():\n",
    "    # Apply the GCN layer to the node features and adjacency matrix\n",
    "    out_feats = layer(node_feats, adj_matrix)\n",
    "    # This step will perform message passing and feature aggregation based on the adjacency matrix.\n",
    "\n",
    "# Print the adjacency matrix to confirm its structure\n",
    "print(\"Adjacency matrix\", adj_matrix)\n",
    "\n",
    "# Print the input node features before the GCN layer operation\n",
    "print(\"Input features\", node_feats)\n",
    "\n",
    "# Print the output node features after applying the GCN layer\n",
    "print(\"Output features\", out_feats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of a Graph Attention Layer (GATLayer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GATLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, c_in, c_out, num_heads=1, concat_heads=True, alpha=0.2):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            c_in - Dimensionality of input features\n",
    "            c_out - Dimensionality of output features\n",
    "            num_heads - Number of attention heads applied in parallel. The output features are\n",
    "                        split equally over the heads if concat_heads=True.\n",
    "            concat_heads - If True, concatenate the output from different heads. Otherwise, average them.\n",
    "            alpha - Negative slope for the LeakyReLU activation function.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.concat_heads = concat_heads\n",
    "        \n",
    "        # c_in: Input feature size per node\n",
    "        # c_out: Output feature size per node\n",
    "        # If the outputs are concatenated, make sure c_out is divisible by num_heads\n",
    "        if self.concat_heads:\n",
    "            assert c_out % num_heads == 0, \"Number of output features must be a multiple of the count of heads.\"\n",
    "            c_out = c_out // num_heads  # Split output features equally among heads\n",
    "\n",
    "        # Linear layer to project node features (c_in) to the desired output size (c_out * num_heads)\n",
    "        self.projection = nn.Linear(c_in, c_out * num_heads)\n",
    "        \n",
    "        # Learnable parameters for attention scores, one set for each head\n",
    "        self.a = nn.Parameter(torch.Tensor(num_heads, 2 * c_out))\n",
    "        \n",
    "        # LeakyReLU activation for computing attention logits\n",
    "        self.leakyrelu = nn.LeakyReLU(alpha)\n",
    "\n",
    "        # Initialize the weights using Xavier initialization to maintain variance across layers\n",
    "        nn.init.xavier_uniform_(self.projection.weight.data, gain=1.414)\n",
    "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
    "\n",
    "    def forward(self, node_feats, adj_matrix, print_attn_probs=False):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            node_feats - Node feature matrix of shape [batch_size, num_nodes, c_in]\n",
    "            adj_matrix - Adjacency matrix with self-connections. Shape: [batch_size, num_nodes, num_nodes]\n",
    "            print_attn_probs - If True, print attention weights during the forward pass for debugging\n",
    "        \"\"\"\n",
    "        batch_size, num_nodes = node_feats.size(0), node_feats.size(1)\n",
    "\n",
    "        # node_feats: Input node features of shape [batch_size, num_nodes, c_in].\n",
    "        # adj_matrix: Adjacency matrix that includes self-loops (connections of nodes with themselves).\n",
    "        \n",
    "        # Apply the linear projection to the node features and reshape them for multiple heads\n",
    "        node_feats = self.projection(node_feats)\n",
    "        node_feats = node_feats.view(batch_size, num_nodes, self.num_heads, -1)\n",
    "\n",
    "        # Attention Computation:\n",
    "        # Get the indices of the edges where there are connections (non-zero values in the adjacency matrix)\n",
    "        edges = adj_matrix.nonzero(as_tuple=False)  # List of edges as (batch_index, node_i, node_j)\n",
    "        \n",
    "        # Flatten node features to make it easier to gather nodes corresponding to the edges\n",
    "        node_feats_flat = node_feats.view(batch_size * num_nodes, self.num_heads, -1)\n",
    "        edge_indices_row = edges[:, 0] * num_nodes + edges[:, 1]\n",
    "        edge_indices_col = edges[:, 0] * num_nodes + edges[:, 2]\n",
    "        \n",
    "        # Concatenate the features for node_i and node_j for all edges\n",
    "        a_input = torch.cat([\n",
    "            torch.index_select(input=node_feats_flat, index=edge_indices_row, dim=0),\n",
    "            torch.index_select(input=node_feats_flat, index=edge_indices_col, dim=0)\n",
    "        ], dim=-1)  # Concatenates features of node pairs along the last dimension\n",
    "\n",
    "        # Compute attention scores (logits) for each head by applying the attention weight matrix `a`\n",
    "        attn_logits = torch.einsum('bhc,hc->bh', a_input, self.a)\n",
    "        attn_logits = self.leakyrelu(attn_logits)  # Apply LeakyReLU non-linearity\n",
    "\n",
    "        # Initialize the attention matrix and fill with a large negative number to apply masking\n",
    "        attn_matrix = attn_logits.new_zeros(adj_matrix.shape + (self.num_heads,)).fill_(-9e15)\n",
    "        \n",
    "        # Assign the computed attention logits to the corresponding edges in the adjacency matrix\n",
    "        attn_matrix[adj_matrix[..., None].repeat(1, 1, 1, self.num_heads) == 1] = attn_logits.reshape(-1)\n",
    "\n",
    "        # Apply softmax to normalize the attention scores for each node's neighbors\n",
    "        attn_probs = F.softmax(attn_matrix, dim=2)\n",
    "        if print_attn_probs:\n",
    "            print(\"Attention probs\\n\", attn_probs.permute(0, 3, 1, 2))  # Debugging output of attention probabilities\n",
    "\n",
    "        # Message Passing:\n",
    "        # Compute the weighted sum of node features based on attention probabilities\n",
    "        node_feats = torch.einsum('bijh,bjhc->bihc', attn_probs, node_feats)\n",
    "\n",
    "        # Concatenation/Averaging:\n",
    "        # If concat_heads=True, the outputs from different attention heads are concatenated. \n",
    "        # Otherwise, they are averaged across heads.\n",
    "        if self.concat_heads:\n",
    "            node_feats = node_feats.reshape(batch_size, num_nodes, -1)  # Flatten to concatenate head outputs\n",
    "        else:\n",
    "            node_feats = node_feats.mean(dim=2)  # Average across heads\n",
    "\n",
    "        return node_feats  # Return the updated node features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Graph Attention Layer with Custom Weights and Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention probs\n",
      " tensor([[[[0.3543, 0.6457, 0.0000, 0.0000],\n",
      "          [0.1096, 0.1450, 0.2642, 0.4813],\n",
      "          [0.0000, 0.1858, 0.2885, 0.5257],\n",
      "          [0.0000, 0.2391, 0.2696, 0.4913]],\n",
      "\n",
      "         [[0.5100, 0.4900, 0.0000, 0.0000],\n",
      "          [0.2975, 0.2436, 0.2340, 0.2249],\n",
      "          [0.0000, 0.3838, 0.3142, 0.3019],\n",
      "          [0.0000, 0.4018, 0.3289, 0.2693]]]])\n",
      "Adjacency matrix tensor([[[1., 1., 0., 0.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [0., 1., 1., 1.],\n",
      "         [0., 1., 1., 1.]]])\n",
      "Input features tensor([[[0., 1.],\n",
      "         [2., 3.],\n",
      "         [4., 5.],\n",
      "         [6., 7.]]])\n",
      "Output features tensor([[[1.2913, 1.9800],\n",
      "         [4.2344, 3.7725],\n",
      "         [4.6798, 4.8362],\n",
      "         [4.5043, 4.7351]]])\n",
      "Attention probs tensor([[[[0.3543, 0.6457, 0.0000, 0.0000],\n",
      "          [0.1096, 0.1450, 0.2642, 0.4813],\n",
      "          [0.0000, 0.1858, 0.2885, 0.5257],\n",
      "          [0.0000, 0.2391, 0.2696, 0.4913]],\n",
      "\n",
      "         [[0.5100, 0.4900, 0.0000, 0.0000],\n",
      "          [0.2975, 0.2436, 0.2340, 0.2249],\n",
      "          [0.0000, 0.3838, 0.3142, 0.3019],\n",
      "          [0.0000, 0.4018, 0.3289, 0.2693]]]])\n",
      "Adjacency matrix tensor([[[1., 1., 0., 0.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [0., 1., 1., 1.],\n",
      "         [0., 1., 1., 1.]]])\n",
      "Input features tensor([[[0., 1.],\n",
      "         [2., 3.],\n",
      "         [4., 5.],\n",
      "         [6., 7.]]])\n",
      "Output features tensor([[[1.2913, 1.9800],\n",
      "         [4.2344, 3.7725],\n",
      "         [4.6798, 4.8362],\n",
      "         [4.5043, 4.7351]]])\n"
     ]
    }
   ],
   "source": [
    "# Initialize a Graph Attention Layer with 2 input features, 2 output features, and 2 attention heads\n",
    "layer = GATLayer(2, 2, num_heads=2)\n",
    "\n",
    "# Set the projection weight to the identity matrix\n",
    "# This ensures that the input features are not transformed (identity transformation)\n",
    "layer.projection.weight.data = torch.Tensor([[1., 0.], [0., 1.]])\n",
    "\n",
    "# Set the projection bias to zeros\n",
    "# No bias is added to the node features in this example\n",
    "layer.projection.bias.data = torch.Tensor([0., 0.])\n",
    "\n",
    "# Set the learnable attention weight parameters (a) for each attention head\n",
    "# These values are arbitrary, to generate diverse attention weights\n",
    "layer.a.data = torch.Tensor([[-0.2, 0.3], [0.1, -0.1]])\n",
    "\n",
    "# Perform a forward pass through the GAT layer without computing gradients\n",
    "# This helps in examining the behavior of the layer without updating any learnable parameters\n",
    "with torch.no_grad():\n",
    "    # Apply the GAT layer on the node features and adjacency matrix\n",
    "    # print_attn_probs=True will output the attention probabilities during the forward pass for debugging purposes\n",
    "    out_feats = layer(node_feats, adj_matrix, print_attn_probs=True)\n",
    "\n",
    "# Print the adjacency matrix for reference\n",
    "print(\"Adjacency matrix\", adj_matrix)\n",
    "\n",
    "# Print the input node features for reference\n",
    "print(\"Input features\", node_feats)\n",
    "\n",
    "# Print the output node features after the GAT layer processes the input\n",
    "print(\"Output features\", out_feats)\n",
    "\n",
    "# Example of attention probabilities for both heads\n",
    "Attention_probs = torch.Tensor([[[[0.3543, 0.6457, 0.0000, 0.0000],\n",
    "                                  [0.1096, 0.1450, 0.2642, 0.4813],\n",
    "                                  [0.0000, 0.1858, 0.2885, 0.5257],\n",
    "                                  [0.0000, 0.2391, 0.2696, 0.4913]],\n",
    "                                 [[0.5100, 0.4900, 0.0000, 0.0000],\n",
    "                                  [0.2975, 0.2436, 0.2340, 0.2249],\n",
    "                                  [0.0000, 0.3838, 0.3142, 0.3019],\n",
    "                                  [0.0000, 0.4018, 0.3289, 0.2693]]]])\n",
    "\n",
    "# Adjacency matrix representing the graph structure\n",
    "Adjacency_matrix = torch.Tensor([[[1., 1., 0., 0.],\n",
    "                                  [1., 1., 1., 1.],\n",
    "                                  [0., 1., 1., 1.],\n",
    "                                  [0., 1., 1., 1.]]])\n",
    "\n",
    "# Input features for each node\n",
    "Input_features = torch.Tensor([[[0., 1.],\n",
    "                                [2., 3.],\n",
    "                                [4., 5.],\n",
    "                                [6., 7.]]])\n",
    "\n",
    "# Output features after applying the GAT layer\n",
    "Output_features = torch.Tensor([[[1.2913, 1.9800],\n",
    "                                 [4.2344, 3.7725],\n",
    "                                 [4.6798, 4.8362],\n",
    "                                 [4.5043, 4.7351]]])\n",
    "\n",
    "print(\"Attention probs\", Attention_probs)\n",
    "print(\"Adjacency matrix\", Adjacency_matrix)\n",
    "print(\"Input features\", Input_features)\n",
    "print(\"Output features\", Output_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing and Importing PyTorch Geometric with Version Compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt to import the PyTorch Geometric library\n",
    "try:\n",
    "    import torch_geometric\n",
    "except ModuleNotFoundError:\n",
    "    # Determine the PyTorch version and CUDA version from the installed PyTorch\n",
    "    # This helps in fetching the correct compatible versions of PyTorch Geometric dependencies\n",
    "    TORCH = torch.__version__.split('+')[0]  # Extract the PyTorch version\n",
    "    CUDA = 'cu' + torch.version.cuda.replace('.', '')  # Extract the CUDA version\n",
    "\n",
    "    # Install the required PyTorch Geometric packages, specifying the correct versions of torch-scatter, torch-sparse, \n",
    "    # torch-cluster, torch-spline-conv, and torch-geometric based on the PyTorch and CUDA version\n",
    "    # Each URL corresponds to a wheel (.whl) file which is a binary package for installation.\n",
    "    !pip install torch-scatter -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
    "    !pip install torch-sparse -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
    "    !pip install torch-cluster -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
    "    !pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
    "    !pip install torch-geometric  # Install the main torch-geometric package\n",
    "\n",
    "# After installation, import PyTorch Geometric modules\n",
    "import torch_geometric\n",
    "\n",
    "# Import specific submodules from torch_geometric\n",
    "# geom_nn provides common neural network layers for graph data (e.g., GCN, GAT, etc.)\n",
    "import torch_geometric.nn as geom_nn\n",
    "\n",
    "# geom_data provides datasets and data structures for working with graph data\n",
    "import torch_geometric.data as geom_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GNN Layer Selection in PyTorch Geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dictionary that maps layer names (strings) to their corresponding graph layer classes in PyTorch Geometric\n",
    "gnn_layer_by_name = {\n",
    "    # \"GCN\" maps to the GCNConv layer from the geom_nn module (Graph Convolutional Network)\n",
    "    \"GCN\": geom_nn.GCNConv,\n",
    "    \n",
    "    # \"GAT\" maps to the GATConv layer from the geom_nn module (Graph Attention Network)\n",
    "    \"GAT\": geom_nn.GATConv,\n",
    "    \n",
    "    # \"GraphConv\" maps to the GraphConv layer from the geom_nn module, which is a variant of GCN with additional functionality\n",
    "    \"GraphConv\": geom_nn.GraphConv\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Accessing the Cora Dataset in PyTorch Geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])\n"
     ]
    }
   ],
   "source": [
    "# Load the Cora dataset using PyTorch Geometric's Planetoid class\n",
    "cora_dataset = torch_geometric.datasets.Planetoid(root=DATASET_PATH, name=\"Cora\")\n",
    "\n",
    "# Print the dataset to confirm it's loaded correctly\n",
    "print(cora_dataset[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the Cora dataset using PyTorch Geometric's Planetoid class\n",
    "cora_dataset = torch_geometric.datasets.Planetoid(root=DATASET_PATH, name=\"Cora\")\n",
    "\n",
    "# Access the first graph in the dataset\n",
    "cora_dataset[0]\n",
    "# This shows the structure: node features (x), edges (edge_index), labels (y), and masks for training/validation/testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flexible Graph Neural Network (GNN) Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a class for a simple Graph Neural Network (GNN) model\n",
    "class GNNModel(nn.Module):\n",
    "\n",
    "    # Initialize the GNNModel with input, hidden, and output dimensions, number of layers, and other parameters\n",
    "    def __init__(self, c_in, c_hidden, c_out, num_layers=2, layer_name=\"GCN\", dp_rate=0.1, **kwargs):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            c_in - Dimension of input features per node\n",
    "            c_hidden - Dimension of hidden features in the intermediate layers\n",
    "            c_out - Dimension of output features (typically the number of classes in classification tasks)\n",
    "            num_layers - Number of graph layers to use in the model\n",
    "            layer_name - Specifies the type of graph layer to use (e.g., GCN, GAT, GraphConv)\n",
    "            dp_rate - Dropout rate for regularization (used in hidden layers)\n",
    "            kwargs - Additional parameters passed to the graph layer (e.g., number of heads for GAT)\n",
    "        \"\"\"\n",
    "        super().__init__()  # Call the parent class (nn.Module) initializer\n",
    "        \n",
    "        # Retrieve the graph layer class from the dictionary using the layer_name (e.g., GCN, GAT)\n",
    "        gnn_layer = gnn_layer_by_name[layer_name]\n",
    "\n",
    "        # Initialize an empty list to store the layers of the GNN\n",
    "        layers = []\n",
    "        in_channels, out_channels = c_in, c_hidden  # Set input and hidden channel dimensions\n",
    "\n",
    "        # Loop to create the intermediate hidden layers (num_layers - 1)\n",
    "        for l_idx in range(num_layers-1):\n",
    "            layers += [\n",
    "                # Add the graph layer with specified input and output dimensions\n",
    "                gnn_layer(in_channels=in_channels,\n",
    "                          out_channels=out_channels,\n",
    "                          **kwargs),\n",
    "                # Add the ReLU activation function to introduce non-linearity\n",
    "                nn.ReLU(inplace=True),\n",
    "                # Add dropout for regularization to avoid overfitting\n",
    "                nn.Dropout(dp_rate)\n",
    "            ]\n",
    "            # Update in_channels to be the size of the hidden layer for the next iteration\n",
    "            in_channels = c_hidden\n",
    "        \n",
    "        # Add the final graph layer (output layer) with output dimension (c_out, usually the number of classes)\n",
    "        layers += [gnn_layer(in_channels=in_channels,\n",
    "                             out_channels=c_out,\n",
    "                             **kwargs)]\n",
    "        \n",
    "        # Store the layers in a ModuleList (a PyTorch container for layers)\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "\n",
    "    # Define the forward pass through the GNN\n",
    "    def forward(self, x, edge_index):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            x - Node feature matrix (input features for each node)\n",
    "            edge_index - Tensor representing the edges in the graph (using PyTorch Geometric notation)\n",
    "        \"\"\"\n",
    "        # Iterate through each layer in the GNN\n",
    "        for l in self.layers:\n",
    "            # Check if the layer is a MessagePassing layer (i.e., a graph layer from PyTorch Geometric)\n",
    "            if isinstance(l, geom_nn.MessagePassing):\n",
    "                # For graph layers, pass both the node features (x) and the edge information (edge_index)\n",
    "                x = l(x, edge_index)\n",
    "            else:\n",
    "                # For non-graph layers (e.g., ReLU, Dropout), just pass the node features\n",
    "                x = l(x)\n",
    "        # Return the output node features after passing through all the layers\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of a Simple Multi-Layer Perceptron (MLP) Model for Node Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a class for a simple Multi-Layer Perceptron (MLP) model\n",
    "class MLPModel(nn.Module):\n",
    "\n",
    "    # Initialize the MLP model with input, hidden, and output dimensions, number of layers, and dropout rate\n",
    "    def __init__(self, c_in, c_hidden, c_out, num_layers=2, dp_rate=0.1):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            c_in - Dimension of input features (number of features per node)\n",
    "            c_hidden - Dimension of hidden layers (number of hidden units)\n",
    "            c_out - Dimension of output features (usually number of classes in classification tasks)\n",
    "            num_layers - Number of hidden layers in the model (default is 2)\n",
    "            dp_rate - Dropout rate for regularization (applied between layers)\n",
    "        \"\"\"\n",
    "        super().__init__()  # Call the parent class (nn.Module) initializer\n",
    "\n",
    "        # Initialize an empty list to store the layers of the MLP\n",
    "        layers = []\n",
    "        in_channels, out_channels = c_in, c_hidden  # Set initial input and hidden layer sizes\n",
    "\n",
    "        # Loop to create the hidden layers (num_layers - 1) with ReLU activation and dropout\n",
    "        for l_idx in range(num_layers-1):\n",
    "            layers += [\n",
    "                # Add a fully connected (linear) layer\n",
    "                nn.Linear(in_channels, out_channels),\n",
    "                # Add ReLU activation to introduce non-linearity\n",
    "                nn.ReLU(inplace=True),\n",
    "                # Add dropout for regularization to prevent overfitting\n",
    "                nn.Dropout(dp_rate)\n",
    "            ]\n",
    "            # Update the input size for the next layer to match the hidden layer size\n",
    "            in_channels = c_hidden\n",
    "\n",
    "        # Add the final fully connected layer (output layer)\n",
    "        layers += [nn.Linear(in_channels, c_out)]\n",
    "\n",
    "        # Use nn.Sequential to create a sequential container for the layers\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    # Define the forward pass through the MLP\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            x - Input features per node (node features or input matrix)\n",
    "        \"\"\"\n",
    "        # Pass the input through the sequence of layers\n",
    "        return self.layers(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Lightning Module for Node-Level GNN and MLP Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a PyTorch Lightning module for node-level tasks using GNNs\n",
    "class NodeLevelGNN(pl.LightningModule):\n",
    "\n",
    "    # Initialize the model with the specified model name and hyperparameters\n",
    "    def __init__(self, model_name, **model_kwargs):\n",
    "        super().__init__()\n",
    "        # Save the hyperparameters (e.g., model configurations) for future reference\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # Initialize the model: if model_name is \"MLP\", create an MLP model; otherwise, use a GNN model\n",
    "        if model_name == \"MLP\":\n",
    "            self.model = MLPModel(**model_kwargs)\n",
    "        else:\n",
    "            self.model = GNNModel(**model_kwargs)\n",
    "        \n",
    "        # Define the loss function for the model, using CrossEntropyLoss for classification tasks\n",
    "        self.loss_module = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Define the forward pass of the model\n",
    "    def forward(self, data, mode=\"train\"):\n",
    "        # Extract node features and edge indices from the data object\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        # Pass the features and edge information through the model (MLP or GNN)\n",
    "        x = self.model(x, edge_index)\n",
    "\n",
    "        # Use the appropriate mask depending on the mode (train, val, or test)\n",
    "        if mode == \"train\":\n",
    "            mask = data.train_mask\n",
    "        elif mode == \"val\":\n",
    "            mask = data.val_mask\n",
    "        elif mode == \"test\":\n",
    "            mask = data.test_mask\n",
    "        else:\n",
    "            assert False, f\"Unknown forward mode: {mode}\"  # Raise an error if the mode is unknown\n",
    "\n",
    "        # Calculate the loss only for the nodes selected by the mask\n",
    "        loss = self.loss_module(x[mask], data.y[mask])\n",
    "        # Calculate accuracy for the selected nodes\n",
    "        acc = (x[mask].argmax(dim=-1) == data.y[mask]).sum().float() / mask.sum()\n",
    "        return loss, acc\n",
    "\n",
    "    # Configure the optimizer for the training process\n",
    "    def configure_optimizers(self):\n",
    "        # Use Stochastic Gradient Descent (SGD) with learning rate, momentum, and weight decay\n",
    "        optimizer = optim.SGD(self.parameters(), lr=0.1, momentum=0.9, weight_decay=2e-3)\n",
    "        return optimizer\n",
    "\n",
    "    # Define the training step, which will be called during each training iteration\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Forward pass to calculate loss and accuracy for the training batch\n",
    "        loss, acc = self.forward(batch, mode=\"train\")\n",
    "        # Log the training loss and accuracy for tracking during training\n",
    "        self.log('train_loss', loss)\n",
    "        self.log('train_acc', acc)\n",
    "        return loss\n",
    "\n",
    "    # Define the validation step, called during validation\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # Forward pass to calculate accuracy for the validation batch\n",
    "        _, acc = self.forward(batch, mode=\"val\")\n",
    "        # Log the validation accuracy for tracking\n",
    "        self.log('val_acc', acc)\n",
    "\n",
    "    # Define the test step, called during testing\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # Forward pass to calculate accuracy for the test batch\n",
    "        _, acc = self.forward(batch, mode=\"test\")\n",
    "        # Log the test accuracy for tracking\n",
    "        self.log('test_acc', acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Function for Node-Level GNN/MLP Models with PyTorch Lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to train the node classifier model (either GNN or MLP)\n",
    "def train_node_classifier(model_name, dataset, **model_kwargs):\n",
    "    # Set a random seed for reproducibility\n",
    "    pl.seed_everything(42)\n",
    "\n",
    "    # Create a data loader for the node-level dataset (batch size is 1 since it's a single graph)\n",
    "    node_data_loader = geom_data.DataLoader(dataset, batch_size=1)\n",
    "\n",
    "    # Define the directory for saving checkpoints during training\n",
    "    root_dir = os.path.join(CHECKPOINT_PATH, \"NodeLevel\" + model_name)\n",
    "    # Create the directory if it doesn't exist\n",
    "    os.makedirs(root_dir, exist_ok=True)\n",
    "\n",
    "    # Create a PyTorch Lightning trainer with checkpointing and GPU/CPU support\n",
    "    trainer = pl.Trainer(default_root_dir=root_dir,\n",
    "                         # Save only the best model based on validation accuracy\n",
    "                         callbacks=[ModelCheckpoint(save_weights_only=True, mode=\"max\", monitor=\"val_acc\")],\n",
    "                         # Use GPU if available, otherwise use CPU\n",
    "                         accelerator=\"gpu\" if str(device).startswith(\"cuda\") else \"cpu\",\n",
    "                         devices=1,  # Use one GPU or CPU\n",
    "                         max_epochs=200,  # Set maximum number of epochs to 200\n",
    "                         enable_progress_bar=False)  # Disable the progress bar since an epoch is just one step\n",
    "\n",
    "    # Disable unnecessary logging of the default hyperparameter metric\n",
    "    trainer.logger._default_hp_metric = None\n",
    "\n",
    "    # Check if a pretrained model exists; if yes, load it and skip training\n",
    "    pretrained_filename = os.path.join(CHECKPOINT_PATH, f\"NodeLevel{model_name}.ckpt\")\n",
    "    if os.path.isfile(pretrained_filename):\n",
    "        # If a pretrained model is found, load it\n",
    "        print(\"Found pretrained model, loading...\")\n",
    "        model = NodeLevelGNN.load_from_checkpoint(pretrained_filename)\n",
    "    else:\n",
    "        # If no pretrained model is found, train a new model\n",
    "        pl.seed_everything()  # Set seed again for reproducibility\n",
    "        # Create a new instance of NodeLevelGNN with the given model name and dataset features\n",
    "        model = NodeLevelGNN(model_name=model_name, c_in=dataset.num_node_features, c_out=dataset.num_classes, **model_kwargs)\n",
    "        # Train the model using the same data loader for training and validation (mask is handled internally)\n",
    "        trainer.fit(model, node_data_loader, node_data_loader)\n",
    "        # Load the best model checkpoint after training\n",
    "        model = NodeLevelGNN.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)\n",
    "\n",
    "    # Test the best model on the test set using the same data loader\n",
    "    test_result = trainer.test(model, node_data_loader, verbose=False)\n",
    "\n",
    "    # Get a single batch from the data loader and move it to the appropriate device (GPU/CPU)\n",
    "    batch = next(iter(node_data_loader))\n",
    "    batch = batch.to(model.device)\n",
    "\n",
    "    # Calculate the accuracy on the training set using the trained model\n",
    "    _, train_acc = model.forward(batch, mode=\"train\")\n",
    "    # Calculate the accuracy on the validation set using the trained model\n",
    "    _, val_acc = model.forward(batch, mode=\"val\")\n",
    "\n",
    "    # Return a dictionary containing the train, validation, and test accuracies\n",
    "    result = {\"train\": train_acc,\n",
    "              \"val\": val_acc,\n",
    "              \"test\": test_result[0]['test_acc']}\n",
    "    return model, result  # Return the trained model and accuracy results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluating MLP on the Cora Dataset with GPU Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "c:\\Users\\Roja\\anaconda3\\Lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\Roja\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\logger_connector\\logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found pretrained model, loading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.0.2 to v2.4.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint D:\\USC_Course\\CSCE 790 Section 007 Neural Networks and Their Applications\\CHECKPOINT_PATH\\NodeLevelMLP.ckpt`\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 97.14%\n",
      "Val accuracy:   54.60%\n",
      "Test accuracy:  60.60%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Roja\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\Roja\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:78: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2708. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n"
     ]
    }
   ],
   "source": [
    "# Define a function to print the test scores (train, validation, and test accuracies)\n",
    "def print_results(result_dict):\n",
    "    # Check if the result dictionary contains the training results\n",
    "    if \"train\" in result_dict:\n",
    "        # Print the training accuracy formatted to 4.2 decimal places\n",
    "        print(f\"Train accuracy: {(100.0*result_dict['train']):4.2f}%\")\n",
    "    \n",
    "    # Check if the result dictionary contains the validation results\n",
    "    if \"val\" in result_dict:\n",
    "        # Print the validation accuracy formatted to 4.2 decimal places\n",
    "        print(f\"Val accuracy:   {(100.0*result_dict['val']):4.2f}%\")\n",
    "    \n",
    "    # Print the test accuracy formatted to 4.2 decimal places\n",
    "    print(f\"Test accuracy:  {(100.0*result_dict['test']):4.2f}%\")\n",
    "\n",
    "# Train a node classifier using the MLP model on the Cora dataset\n",
    "node_mlp_model, node_mlp_result = train_node_classifier(\n",
    "    model_name=\"MLP\",              # Specify the model type as MLP (Multi-Layer Perceptron)\n",
    "    dataset=cora_dataset,          # Use the Cora dataset for node classification\n",
    "    c_hidden=16,                   # Set the hidden layer size to 16\n",
    "    num_layers=2,                  # Specify that the MLP should have 2 layers\n",
    "    dp_rate=0.1                    # Set the dropout rate to 0.1 for regularization\n",
    ")\n",
    "\n",
    "# Print the train, validation, and test accuracies for the trained MLP model\n",
    "print_results(node_mlp_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "c:\\Users\\Roja\\anaconda3\\Lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Lightning automatically upgraded your loaded checkpoint from v1.0.2 to v2.4.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint D:\\USC_Course\\CSCE 790 Section 007 Neural Networks and Their Applications\\CHECKPOINT_PATH\\NodeLevelGNN.ckpt`\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found pretrained model, loading...\n",
      "Train accuracy: 100.00%\n",
      "Val accuracy:   78.60%\n",
      "Test accuracy:  82.40%\n"
     ]
    }
   ],
   "source": [
    "# Train a node classifier using a Graph Neural Network (GNN) model on the Cora dataset\n",
    "node_gnn_model, node_gnn_result = train_node_classifier(\n",
    "    model_name=\"GNN\",             # Specify the model type as GNN (Graph Neural Network)\n",
    "    layer_name=\"GCN\",             # Specify the graph layer type as GCN (Graph Convolutional Network)\n",
    "    dataset=cora_dataset,         # Use the Cora dataset for node classification\n",
    "    c_hidden=16,                  # Set the hidden layer size to 16\n",
    "    num_layers=2,                 # Specify that the GNN should have 2 layers\n",
    "    dp_rate=0.1                   # Set the dropout rate to 0.1 for regularization\n",
    ")\n",
    "\n",
    "# Print the train, validation, and test accuracies for the trained GNN model\n",
    "print_results(node_gnn_result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the MUTAG Dataset for Graph Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data object: Data(x=[3371, 7], edge_index=[2, 7442], edge_attr=[7442, 4], y=[188])\n",
      "Length: 188\n",
      "Average label: 0.66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Roja\\anaconda3\\Lib\\site-packages\\torch_geometric\\data\\in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Load the MUTAG dataset using PyTorch Geometric's TUDataset class\n",
    "tu_dataset = torch_geometric.datasets.TUDataset(root=DATASET_PATH, name=\"MUTAG\")\n",
    "\n",
    "# Print some basic information about the dataset\n",
    "print(\"Data object:\", tu_dataset.data)  # Print the structure of the data object storing the graphs\n",
    "print(\"Length:\", len(tu_dataset))       # Print the number of graphs in the dataset (188 graphs)\n",
    "# Calculate and print the average label (percentage of graphs labeled with 1)\n",
    "print(f\"Average label: {tu_dataset.data.y.float().mean().item():4.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the MUTAG Dataset into Training and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a manual random seed to ensure reproducibility when shuffling the dataset\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Shuffle the dataset so that graphs are randomly ordered\n",
    "tu_dataset.shuffle()\n",
    "\n",
    "# Split the dataset into training (first 150 graphs) and test (remaining 38 graphs) sets\n",
    "train_dataset = tu_dataset[:150]  # Select the first 150 graphs as the training set\n",
    "test_dataset = tu_dataset[150:]   # Select the remaining graphs as the test set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Data Loaders for Batching Multiple Graphs Efficiently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataLoader for the training dataset\n",
    "graph_train_loader = geom_data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Create a DataLoader for the validation dataset (can use test dataset as validation due to small size)\n",
    "graph_val_loader = geom_data.DataLoader(test_dataset, batch_size=64) \n",
    "\n",
    "# Create a DataLoader for the test dataset\n",
    "graph_test_loader = geom_data.DataLoader(test_dataset, batch_size=64) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting a Batch of Graphs from the Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: DataBatch(edge_index=[2, 1512], x=[687, 7], edge_attr=[1512, 4], y=[38], batch=[687], ptr=[39])\n",
      "Labels: tensor([1, 1, 1, 0, 0, 0, 1, 1, 1, 0])\n",
      "Batch indices: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "# Get the next batch of graphs from the test data loader\n",
    "batch = next(iter(graph_test_loader))\n",
    "\n",
    "# Print the entire batch object to inspect its structure\n",
    "print(\"Batch:\", batch)\n",
    "\n",
    "# Print the first 10 graph labels from the batch (y represents the graph labels)\n",
    "print(\"Labels:\", batch.y[:10])\n",
    "\n",
    "# Print the batch indices that map nodes to their corresponding graphs in the batch\n",
    "print(\"Batch indices:\", batch.batch[:40])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph-Level GNN Model with Global Pooling for Graph Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a graph-level GNN model class for graph classification\n",
    "class GraphGNNModel(nn.Module):\n",
    "\n",
    "    # Initialize the model with input, hidden, output dimensions, dropout, and additional GNN arguments\n",
    "    def __init__(self, c_in, c_hidden, c_out, dp_rate_linear=0.5, **kwargs):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            c_in - Dimension of input features\n",
    "            c_hidden - Dimension of hidden features\n",
    "            c_out - Dimension of output features (usually the number of classes)\n",
    "            dp_rate_linear - Dropout rate before the linear layer (higher than inside GNN)\n",
    "            kwargs - Additional arguments for the GNN model\n",
    "        \"\"\"\n",
    "        super().__init__()  # Call the parent class (nn.Module) initializer\n",
    "        # Define the GNN model using the GNNModel class and pass input, hidden, and output dimensions\n",
    "        self.GNN = GNNModel(c_in=c_in,\n",
    "                            c_hidden=c_hidden,\n",
    "                            c_out=c_hidden,  # Intermediate hidden output, not the final output yet\n",
    "                            **kwargs)  # Pass additional keyword arguments if any\n",
    "\n",
    "        # Define the head of the model, including dropout and a linear layer for the final classification\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Dropout(dp_rate_linear),  # Dropout for regularization before the linear layer\n",
    "            nn.Linear(c_hidden, c_out)   # Linear layer for mapping hidden features to output classes\n",
    "        )\n",
    "\n",
    "    # Define the forward pass for the model\n",
    "    def forward(self, x, edge_index, batch_idx):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            x - Input features per node\n",
    "            edge_index - List of vertex index pairs representing the edges in the graph (PyTorch Geometric notation)\n",
    "            batch_idx - Index of batch element for each node (used for pooling nodes per graph)\n",
    "        \"\"\"\n",
    "        # Pass the node features and edge information through the GNN model\n",
    "        x = self.GNN(x, edge_index)\n",
    "\n",
    "        # Apply global mean pooling over the nodes for each graph, combining node features into a single graph feature\n",
    "        x = geom_nn.global_mean_pool(x, batch_idx)  # Average pooling for aggregating node features at the graph level\n",
    "\n",
    "        # Pass the pooled features through the linear head for final prediction (classification)\n",
    "        x = self.head(x)\n",
    "\n",
    "        # Return the output (graph-level prediction)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph-Level GNN Model Training Using PyTorch Lightning for Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a PyTorch Lightning module for graph-level tasks using a GNN\n",
    "class GraphLevelGNN(pl.LightningModule):\n",
    "\n",
    "    # Initialize the module with hyperparameters and model configuration\n",
    "    def __init__(self, **model_kwargs):\n",
    "        super().__init__()\n",
    "        # Save the hyperparameters (such as model configurations) for future reference\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # Initialize the graph-level GNN model\n",
    "        self.model = GraphGNNModel(**model_kwargs)\n",
    "\n",
    "        # Use Binary Cross Entropy Loss for binary classification, Cross Entropy for multi-class classification\n",
    "        self.loss_module = nn.BCEWithLogitsLoss() if self.hparams.c_out == 1 else nn.CrossEntropyLoss()\n",
    "\n",
    "    # Define the forward pass of the model\n",
    "    def forward(self, data, mode=\"train\"):\n",
    "        # Extract node features, edge indices, and batch indices from the data object\n",
    "        x, edge_index, batch_idx = data.x, data.edge_index, data.batch\n",
    "        # Pass the features through the GNN model\n",
    "        x = self.model(x, edge_index, batch_idx)\n",
    "        # Remove extra dimensions if necessary\n",
    "        x = x.squeeze(dim=-1)\n",
    "\n",
    "        # Determine predictions based on the output\n",
    "        if self.hparams.c_out == 1:  # Binary classification case\n",
    "            preds = (x > 0).float()  # Convert logits to binary predictions\n",
    "            data.y = data.y.float()  # Ensure target labels are floats\n",
    "        else:  # Multi-class classification case\n",
    "            preds = x.argmax(dim=-1)  # Use argmax to get predicted class for each graph\n",
    "\n",
    "        # Calculate the loss using the appropriate loss function\n",
    "        loss = self.loss_module(x, data.y)\n",
    "        # Calculate accuracy\n",
    "        acc = (preds == data.y).sum().float() / preds.shape[0]\n",
    "        return loss, acc\n",
    "\n",
    "    # Configure the optimizer for training\n",
    "    def configure_optimizers(self):\n",
    "        # Use AdamW optimizer with a learning rate of 1e-2 and no weight decay (since the dataset is small)\n",
    "        optimizer = optim.AdamW(self.parameters(), lr=1e-2, weight_decay=0.0)\n",
    "        return optimizer\n",
    "\n",
    "    # Define the training step\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Perform a forward pass and compute loss and accuracy for the training batch\n",
    "        loss, acc = self.forward(batch, mode=\"train\")\n",
    "        # Log training loss and accuracy\n",
    "        self.log('train_loss', loss)\n",
    "        self.log('train_acc', acc)\n",
    "        return loss\n",
    "\n",
    "    # Define the validation step\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # Perform a forward pass and compute accuracy for the validation batch\n",
    "        _, acc = self.forward(batch, mode=\"val\")\n",
    "        # Log validation accuracy\n",
    "        self.log('val_acc', acc)\n",
    "\n",
    "    # Define the test step\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # Perform a forward pass and compute accuracy for the test batch\n",
    "        _, acc = self.forward(batch, mode=\"test\")\n",
    "        # Log test accuracy\n",
    "        self.log('test_acc', acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Graph-Level GNN with PyTorch Lightning and Model Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_graph_classifier(model_name, **model_kwargs):\n",
    "    pl.seed_everything(42)\n",
    "\n",
    "    # Create a PyTorch Lightning trainer with the generation callback\n",
    "    root_dir = os.path.join(CHECKPOINT_PATH, \"GraphLevel\" + model_name)\n",
    "    os.makedirs(root_dir, exist_ok=True)\n",
    "    \n",
    "    trainer = pl.Trainer(\n",
    "        default_root_dir=root_dir,\n",
    "        callbacks=[ModelCheckpoint(save_weights_only=True, mode=\"max\", monitor=\"val_acc\")],\n",
    "        accelerator=\"gpu\" if str(device).startswith(\"cuda\") else \"cpu\",\n",
    "        devices=1,\n",
    "        max_epochs=500,\n",
    "        enable_progress_bar=False\n",
    "    )\n",
    "    trainer.logger._default_hp_metric = None  # Optional logging argument that we don't need\n",
    "\n",
    "    # Check if pretrained model exists, load it if available\n",
    "    pretrained_filename = os.path.join(CHECKPOINT_PATH, f\"GraphLevel{model_name}.ckpt\")\n",
    "    if os.path.isfile(pretrained_filename):\n",
    "        print(\"Found pretrained model, loading...\")\n",
    "        model = GraphLevelGNN.load_from_checkpoint(pretrained_filename)\n",
    "    else:\n",
    "        pl.seed_everything(42)\n",
    "        model = GraphLevelGNN(\n",
    "            c_in=tu_dataset.num_node_features,\n",
    "            c_out=1 if tu_dataset.num_classes == 2 else tu_dataset.num_classes,\n",
    "            **model_kwargs\n",
    "        )\n",
    "        trainer.fit(model, graph_train_loader, graph_val_loader)\n",
    "        model = GraphLevelGNN.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)\n",
    "\n",
    "    # Test best model on validation and test set\n",
    "    train_result = trainer.test(model, graph_train_loader, verbose=False)  # Test on training data\n",
    "    test_result = trainer.test(model, graph_test_loader, verbose=False)    # Test on test data\n",
    "\n",
    "    # Ensure both test and train accuracy are logged in the result\n",
    "    result = {\"test\": test_result[0]['test_acc'], \"train\": train_result[0].get('test_acc', 0)}\n",
    "    return model, result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Testing Graph-Level GNN with GraphConv Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Lightning automatically upgraded your loaded checkpoint from v1.0.2 to v2.4.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint D:\\USC_Course\\CSCE 790 Section 007 Neural Networks and Their Applications\\CHECKPOINT_PATH\\GraphLevelGraphConv.ckpt`\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "c:\\Users\\Roja\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:475: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\Roja\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:78: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found pretrained model, loading...\n",
      "Train performance: 93.2765%\n",
      "Test performance: 92.1053%\n"
     ]
    }
   ],
   "source": [
    "# Train the graph classification model using the GraphConv layer\n",
    "model, result = train_graph_classifier(\n",
    "    model_name=\"GraphConv\",       # Specify the model name as GraphConv\n",
    "    c_hidden=256,                 # Number of hidden units in the model\n",
    "    layer_name=\"GraphConv\",       # Use the GraphConv layer for the GNN\n",
    "    num_layers=3,                 # Define the number of layers in the model (3 layers)\n",
    "    dp_rate_linear=0.5,           # Dropout rate of 50% before the final linear layer\n",
    "    dp_rate=0.0                   # Dropout rate of 0% in the GNN layers\n",
    ")\n",
    "\n",
    "# Print the training and test performance\n",
    "print(f\"Train performance: {100.0*result['train']:.4f}%\")  # Print the training accuracy\n",
    "print(f\"Test performance: {100.0*result['test']:.4f}%\")    # Print the test accuracy\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
